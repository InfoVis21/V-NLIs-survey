<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Towards Natural Language Interfaces for Data Visualization</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
    integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
  <link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css"
    integrity="sha384-AYmEC3Yw5cVb3ZcuHtOA93w35dYTsvhLPVnYs9eStHfGJvOvKxVfELGroGkvsg+p" crossorigin="anonymous" />
  <link href="css/style.css" rel="stylesheet">

  <!-- JavaScript -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
    integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.bundle.min.js"
    integrity="sha384-LtrjvnR4Twt/qOuYxE721u19sVFLVSA4hf/rRt6PrZTmiPltdZcI7q7PXQBYTKyf"
    crossorigin="anonymous"></script>

</head>

<body>

  <!-- Navigation -->
  <nav id="pageHeader" class="navbar navbar-expand-lg navbar-dark">
    <div class="container">
      <h3>Towards Natural Language Interfaces for Data Visualization: A Survey</h3>
      <!-- <h5><a id="githubLink" href="https://github.com/InfoVis21/V-NLIs-survey"><i
            class="fab fa-github"></i>GitHub</a>
      </h5> -->
    </div>
    updated: 2021-12-5
  </nav>

  <!-- Page Content -->
  <div id="mainBody" class="container">
    <div class="row">
      <div class="col-md-12">
        <p>
          Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. 
          It enables users to focus on their tasks rather than worrying about operating the interface to visualization tools. 
          In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed both within academic research and commercial software, especially in recent years. 
          In our survey, we conduct a comprehensive review of the existing V-NLIs. 
          In order to classify each paper, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and presentation.
          Finally, we also shed light on several promising directions for future work in the community.
        </p>
        <!-- <p>
          This website is a supplementary material for our paper, providing details of our survey and evaluation.
        </p> -->
        <!-- <OBJECT src="pipeline.pdf" type="application/pdf" width="100%" height="100%" internalinstanceid="81 /> -->
      </div>
      <img src="pipeline.png" width="100%"/>
    </div>

    <hr />
    <div>
      <h5><i class="fa fa-table"></i> Survey (Click to <a
          href="https://docs.google.com/spreadsheets/d/1yfQpfd4kc84mrfIbHUSw92LWKRRMXQJ0vN2LTSnDVME/edit?usp=sharing"
          target="blank">Google Sheet</a>)</h5>
      <p>
        The table lists the details of related works, including NLP libraries applied in the system, chart types
        supported, visualization recommendation algorithm adopted, and various characteristics in V-NLIs.
      </p>
      <div style="width: 100%;height: 800px;">
        <iframe id="datasetPreview"
          src="https://docs.google.com/spreadsheets/d/1yfQpfd4kc84mrfIbHUSw92LWKRRMXQJ0vN2LTSnDVME/edit?usp=sharing&amp;single=true&amp;widget=true&amp;headers=false"
          width="100%" height="100%"></iframe>
      </div>
      <br>

      <!-- <p>
        <u>Column descriptions:</u>
      <ul>
        <li><b>Name</b>: Name of the system or technology. </li>
        <li><b>Publication</b>: Journal or conference of publication with publishing year. </li>
        <li><b>NLP Library</b>: NLP libraries applied in the system. </li>
        <li><b>Chart Type</b>: Various types of chart supported, including Bar(B), Table(Ta), Infographic(I),
          Scatter(S), Radar(R), Line(L), Pie(P), Boxplot(Bo), Icon(Ic), Map(M), Heatmap(H), Timeline(Tl), Area(A),
          Network(N), Tree(Tr), Strip(St), Donut(D), Gantt(G), Word clouds(Wc), Force graph(Fg), Range(Ra), Unit column
          charts(Uc), and Graphics(Gr). </li>
        <li><b>recommendation Algorithm</b>: Algorithm used for generating visualization recommendations. </li>
        <li><b>Ambiguity Widget</b>: Whether to offer ambiguity widget for users. </li>
        <li><b>Defaults for Underspecifition</b>: Whether to provide defaults for underspecified utterances.</li>
        <li><b>Conversation</b>: Whether to support multi-turn conversation. </li>
        <li><b>Autocompletion</b>: Whether to consider semantics of data columns.</li>
        <li><b>Semantics Parsing</b>: Whether to support annotation on charts.</li>
        <li><b>StoryTelling</b>: Whether to support data storytelling. </li>
        <li><b>Annotation</b>: Whether to support autocompletion when users input queries.</li>
        <li><b>Multimodel</b>: Whether to provide users with multimodal interfaces other than WIMP and NL, such as pen
          and touch. </li>
        <li><b>WIMP</b>: Whether to offer WIMP manipulations for users.</li>
        <li><b>Multi-Recs</b>: Whether to generate multiple visualizations for a query.</li>
        <li><b>Website</b>: Project website if opensourced.</li>
      </ul>
      </p> -->
    </div>


    <hr />
    <div>
      <h5><i class="fa fa-table"></i> Available Datasets for V-NLIs</h5>

      <table  border="1" align="center" width=100% >
        <tr>
            <td align="center"><b>Name</b></td>
            <td align="center"><b>Publication</b></td>
            <td align="center"><b>NL queries Num</b></td>
            <td align="center"><b>Data tables Num</b></td>
            <td align="center"><b>Benchmark</b></td>
            <td align="center"><b>Other Contributions</b></td>
            <td align="center"><b>Website</b></td>
        </tr>
        <tr>
            <td align="center">VisQA</td>
            <td align="center">CHI'20</td>
            <td align="center">629</td>
            <td align="center">52</td>
            <td align="center">×</td>
            <td align="center">VQA with explanations</td>
            <td><a href="url">https://github.com/dhkim16/VisQA-release</a></td>
        </tr>
        <tr>
            <td align="center">Quda</td>
            <td align="center">arXiv'20</td>
            <td align="center">14,035</td>
            <td align="center">36</td>
            <td align="center">×</td>
            <td align="center">Three Quda applications</td>
            <td><a href="url">https://freenli.github.io/quda/</a></td>
        </tr>
        <tr>
            <td align="center">NLV</td>
            <td align="center">CHI'21</td>
            <td align="center">893</td>
            <td align="center">3</td>
            <td align="center">√</td>
            <td align="center">Characterization of utterances</td>
            <td><a href="url">https://nlvcorpus.github.io/</a></td>
        </tr>
        <tr>
            <td align="center">nvBench</td>
            <td align="center">SIGMOD'21</td>
            <td align="center">25,750</td>
            <td align="center">780</td>
            <td align="center">√</td>
            <td align="center">NL2SQL-to-NL2VIS and SEQ2VIS</td>
            <td><a href="url">https://github.com/TsinghuaDatabaseGroup/nvBench</a></td>
        </tr>
      </table>



      <li><b><a href="https://github.com/TsinghuaDatabaseGroup/nvBench" target="blank">nvBench</a></b>
        <p>nvBench is a large dataset for complex and cross-domain NL2VIS task, which covers 105 domains, supports seven common types of visualizations, and contains 25,750 (NL, VIS) pairs. This repository contains the corpus of NL2VIS, with JSON format and Vega-Lite format.
        </p>
      <li><b><a href="https://nlvcorpus.github.io/" target="blank">NLV</a></b>
        <p>There is a lack of empirical understanding of how people specify visualizations through natural language.
          Researchers of NLV conducted an online study (N = 102), showing participants a series of visualizations and
          asking them to provide utterances they would pose to generate the displayed charts. From the responses, they
          curated a dataset of 893 utterances and characterized the utterances according to (1) their phrasing (e.g.,
          commands, queries, questions) and (2) the information they contained (e.g., chart types, data aggregations).
        </p>
      <li><b><a href="https://freenli.github.io/quda/" target="blank">Quda</a></b>
        <p>Quda aims to help V-NLIs recognize analytic tasks from free-form natural language by training and evaluating
          cutting-edge multi-label classification models. The dataset contains 14035 diverse user queries, and each is
          annotated with one or multiple analytic tasks. Quda achieves this goal by first gathering seed queries with
          data analysts and then employing extensive crowd force for paraphrase generation and validation. This work is
          the first attempt to construct a large-scale corpus for recognizing analytic tasks.
        </p>
      <li><b><a href="https://github.com/dhkim16/VisQA-release" target="blank">VisQA</a></b>
        <p>People often use charts to analyze data, answer questions and explain their answers to others. In a formative
          study, Kim et al. found that such human-generated questions and explanations commonly refer to visual features
          of charts. Based on this study, they developed an automatic chart question answering pipeline that generates
          visual explanations describing how the answer was obtained. During the study, they showed people various
          bar charts and line charts, and collected questions participants posed about the charts along with their
          answers and explanations to those questions.
        </p>
    </div>


    <hr />
    <div>
      <h5><i class="fa fa-table"></i> Test cases and evaluation results (Click to <a
          href="https://docs.google.com/spreadsheets/d/1EPZkPblZ6zCuuG2k1PIVAZFiSx2qUxIf_4drSmJudCQ/edit?usp=sharing"
          target="blank">Google Sheet</a>)</h5>
      <p>
        To evaluate the state-of-the-art open-source V-NLIs (FlowSense, NL4DV, Microsoft's Power BI and Tableau's Ask
        Data), we defined a two-dimensional space that varies in task and information level for visualization-oriented
        NLIs. At the task-level, we adopted the widely recognized ten low-level tasks (e.g., Characterize Distribution
        and Find Extremum) proposed by Amar et al. As to information-level, human language is complex and can be divided
        into vague and specific according to utterance information. Although the communication of information between
        people is very smooth and unimpeded, whether itis vague or specific, it is very different for machines to parse
        the two types of utterance. Based on the space, we designed a set of test cases in each scenario as shown in the
        yellow-colored columns below.
      </p>
      <p>
        The evaluation results are shown in pink-colored columns, bath generated visualizations and judged results. The
        results can be divided into three catatories: (a) The system cannot parse the input query, and no result was
        produced (marked as “-”); (b) The system can generate visualization but cannot meet the expected demand (marked
        as “×”); (c) The generated visualization can correctly extract the target data attributes and well reflect the
        user's analytic task (marked as “√”).
      </p>
      <div style="width: 100%;height: 1000px;">
        <iframe id="datasetPreview"
          src="https://docs.google.com/spreadsheets/d/1EPZkPblZ6zCuuG2k1PIVAZFiSx2qUxIf_4drSmJudCQ/edit?usp=sharing&amp;single=true&amp;widget=true&amp;headers=false"
          width="100%" height="100%"></iframe>
      </div>
      <p>
        Column descriptions:
      <ul>
        <li><b>Task</b>: Ten low-level tasks of analytic activity in information visualization at task-level.</li>
        <li><b>Dataset</b>: The dataset the designed queries are issued in the context of. The <a
            href="https://github.com/InfoVis21/V-NLIs-survey/tree/master/Dataset" target="blank">ten datasets</a>
          used during the evaluation can be found on the GitHub repository.</li>
        <li><b>Query type</b>: Whether the query is specific or vague.</li>
        <li><b>Query</b>: Query inputted to the systems.</li>
        <li><b>Target attributes</b>: Data attributes that should be anaylyzed in the query.</li>
        <li><b>Evaluation results titled by names of four state-of-the-art systems</b>: The <a
            href="https://github.com/InfoVis21/V-NLIs-survey/tree/master/Evaluation%20vis" target="blank">generated
            visualizations</a>
          during the evaluation can be found on the GitHub repository. The mark types in the table above were finally
          determined after discussion by multiple scholars. Fortunately, in general, these results were quite different
          and easy to distinguish which is better. </li>
      </ul>
      </p>
      <br>
    </div>

    <hr />


  </div>

</body>

</html>
